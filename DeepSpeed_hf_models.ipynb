{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepSpeed_hf_models",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install all you need for train"
      ],
      "metadata": {
        "id": "WoqWvSTz6f7S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo1eyZX_4n44"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers\n",
        "!pip install mpi4py\n",
        "!pip install transformers[deepspeed]\n",
        "!apt-get install libaio-dev"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "id": "FWUIjhD7624u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deepspeed config, usefull for any model\n",
        "\n",
        "Read more how wtf all parametrs mean read [here](https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training\n",
        ")"
      ],
      "metadata": {
        "id": "N4QVUHPz6jYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ds_config_gpt_j.json\n",
        "{\n",
        "  \"train_batch_size\": 15,\n",
        "  \"fp16\": {\n",
        "    \"enabled\": true,\n",
        "    \"min_loss_scale\": 1,\n",
        "    \"opt_level\": \"O3\"\n",
        "  },\n",
        "  \"zero_optimization\": {\n",
        "    \"stage\": 3,\n",
        "    \"offload_param\": {\n",
        "      \"device\": \"nvme\",\n",
        "      \"nvme_path\": \"/home/deepschneider/deepspeed\",\n",
        "      \"buffer_count\": 5,\n",
        "      \"buffer_size\": 1e8,\n",
        "      \"max_in_cpu\": 1e9\n",
        "    },\n",
        "    \"offload_optimizer\": {\n",
        "      \"device\": \"nvme\",\n",
        "      \"nvme_path\": \"/home/deepschneider/deepspeed\",\n",
        "      \"buffer_count\": 4,\n",
        "      \"pipeline_read\": false,\n",
        "      \"pipeline_write\": false,\n",
        "      \"pin_memory\": true\n",
        "    },\n",
        "    \"allgather_partitions\": true,\n",
        "    \"allgather_bucket_size\": 5e8,\n",
        "    \"contiguous_gradients\": true,\n",
        "    \"overlap_comm\": true,\n",
        "    \"aio\": {\n",
        "      \"block_size\": 1048576,\n",
        "      \"queue_depth\": 8,\n",
        "      \"thread_count\": 1,\n",
        "      \"single_submit\": false,\n",
        "      \"overlap_events\": true\n",
        "    }\n",
        "  },\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"AdamW\",\n",
        "    \"params\": {\n",
        "      \"lr\": 5e-05,\n",
        "      \"betas\": [\n",
        "        0.9,\n",
        "        0.999\n",
        "      ],\n",
        "      \"eps\": 1e-08\n",
        "    }\n",
        "  },\n",
        "  \"scheduler\": {\n",
        "    \"type\": \"WarmupLR\",\n",
        "    \"params\": {\n",
        "      \"warmup_min_lr\": 0,\n",
        "      \"warmup_max_lr\": 5e-05,\n",
        "      \"warmup_num_steps\": 100\n",
        "    }\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "fipvBN1549_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown godbless 2ch data\n",
        "!gdown https://drive.google.com/uc?id=1sYdutKVcXgcg-lTtanv1WkGQ4cTYNz0d"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6RrjDtto5UN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown simple data preprocessing\n",
        "import re\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "from io import StringIO\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "class MLStripper(HTMLParser):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.reset()\n",
        "        self.strict = False\n",
        "        self.convert_charrefs= True\n",
        "        self.text = StringIO()\n",
        "    def handle_data(self, d):\n",
        "        self.text.write(d)\n",
        "    def get_data(self):\n",
        "        return self.text.getvalue()\n",
        "\n",
        "def strip_tags(html):\n",
        "    s = MLStripper()\n",
        "    s.feed(html)\n",
        "    return s.get_data()\n",
        "\n",
        "\n",
        "texts = pd.read_csv('/content/2ch.csv')['post']\n",
        "\n",
        "def build_text_files(datas, dest_path):\n",
        "    f = open(dest_path, 'w')\n",
        "    data = ''\n",
        "    for text in datas:\n",
        "      if type(text)==str:\n",
        "        if len(text)>10:\n",
        "          post = strip_tags(text)\n",
        "          post = post.replace('br','').replace('<span class=\"spoiler\">','').replace('</strong>','').replace('<strong>','').replace('<span class=\"unkfunc\">&g','')\n",
        "          data += post \n",
        "        \n",
        "        \n",
        "        \n",
        "    f.write(data)\n",
        "\n",
        "train, test = train_test_split(texts,test_size=0.15)\n",
        "\n",
        "build_text_files(train,'train_dataset.txt')\n",
        "build_text_files(test,'test_dataset.txt')\n",
        "\n",
        "print(\"Train dataset length: \"+str(len(train)))\n",
        "print(\"Test dataset length: \"+ str(len(test)))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TgT6EqWn5G3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "MASTER port should be open if train with ddp\n",
        "RAnk - main gpu\n",
        "\n",
        "\"\"\"\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '9994'\n",
        "os.environ['RANK'] = \"0\"\n",
        "os.environ['LOCAL_RANK'] = \"0\"# for ddp\n",
        "os.environ['WORLD_SIZE'] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" #uncoment for large files\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM\n",
        "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/rugpt3large_based_on_gpt2\", \n",
        "                                          bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"sberbank-ai/rugpt3large_based_on_gpt2\").cuda()\n",
        "\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_path = 'train_dataset.txt'\n",
        "test_path = 'test_dataset.txt'\n",
        "\n",
        "def load_dataset(train_path,test_path,tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "\n",
        "    test_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=128)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset,test_dataset,data_collator\n",
        "\n",
        "\n",
        "\n",
        "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(output_dir='deepspeed',\n",
        "                                  num_train_epochs=5, \n",
        "                                  logging_steps=300, \n",
        "                                  save_steps=3000,\n",
        "                                  per_device_train_batch_size=15,\n",
        "                                  per_device_eval_batch_size=15,\n",
        "                                  warmup_steps=100,\n",
        "                                  weight_decay=0.01, \n",
        "                                  \n",
        "                                  fp16=True,\n",
        "                                  #warmup_steps=10,\n",
        "                                  #weight_decay=0.01,  \n",
        "                                  #fp16=True, \n",
        "                                  #fp16_opt_level='O1', not useful beacuse deepspeed\n",
        "                                  report_to=\"wandb\",\n",
        "                                  deepspeed='ds_config_gpt_j.json')\n",
        "trainer = Trainer(model=model, args=training_args, \n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ngLKjg335AGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py"
      ],
      "metadata": {
        "id": "pB8WpDGF6TKm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}